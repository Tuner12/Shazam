initializing dataset
args.batch_size 512
Loading model checkpoint: phikon_v2
  0%|          | 0/31 [00:00<?, ?it/s]
progress: 0/31
TCGA-A3-3322-01Z-00-DX1.9d583339-7726-4eab-a3fb-1a1f2abf8fcf
skipped TCGA-A3-3322-01Z-00-DX1.9d583339-7726-4eab-a3fb-1a1f2abf8fcf

progress: 1/31
TCGA-A3-3357-01Z-00-DX1.b85a8293-7b4f-4a8a-b799-77d7d7b39b3e
skipped TCGA-A3-3357-01Z-00-DX1.b85a8293-7b4f-4a8a-b799-77d7d7b39b3e

progress: 2/31
TCGA-A3-3385-01Z-00-DX1.c7b1827d-aa03-4622-a736-e6f27992b7cd
skipped TCGA-A3-3385-01Z-00-DX1.c7b1827d-aa03-4622-a736-e6f27992b7cd

progress: 3/31
TCGA-AK-3436-01Z-00-DX1.E8C5CA5A-4DA0-4F66-818E-6C56B22C1EC1
skipped TCGA-AK-3436-01Z-00-DX1.E8C5CA5A-4DA0-4F66-818E-6C56B22C1EC1

progress: 4/31
TCGA-AS-3777-01Z-00-DX1.3c2ee8f6-91f4-46e6-a84b-676d8560f3d4
skipped TCGA-AS-3777-01Z-00-DX1.3c2ee8f6-91f4-46e6-a84b-676d8560f3d4

progress: 5/31
TCGA-B0-4710-01Z-00-DX1.e1440c30-b28d-42a8-b126-5abab7e0e3b2
downsample [1. 1.]
downsampled_level_dim [78658 76621]
level_dim [78658 76621]
name TCGA-B0-4710-01Z-00-DX1.e1440c30-b28d-42a8-b126-5abab7e0e3b2
patch_level 0
patch_size 512
save_path ../survival_analysis/TCGA_KIRC_PATCH_DIR40/patches

feature extraction settings
transformations:  Compose(
    Resize(size=224, interpolation=bilinear, max_size=None, antialias=True)
    ToTensor()
    Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
)
processing a total of 29 batches

  0%|          | 0/29 [00:00<?, ?it/s][A
  3%|â–Ž         | 1/29 [00:27<12:38, 27.09s/it][A
  7%|â–‹         | 2/29 [00:30<05:58, 13.27s/it][A
 10%|â–ˆ         | 3/29 [00:34<03:50,  8.86s/it][A
 14%|â–ˆâ–        | 4/29 [00:37<02:49,  6.79s/it][A
 17%|â–ˆâ–‹        | 5/29 [00:41<02:17,  5.73s/it][A
 21%|â–ˆâ–ˆ        | 6/29 [00:45<01:55,  5.02s/it][A 21%|â–ˆâ–ˆ        | 6/29 [00:45<02:54,  7.57s/it]
 16%|â–ˆâ–Œ        | 5/31 [00:45<03:56,  9.09s/it]
Traceback (most recent call last):
  File "/ailab/public/pjlab-smarthealth03/leiwenhui/YushengTan/CLAM/extract_multi_features.py", line 120, in <module>
    output_file_path = compute_w_loader(output_path, loader = loader, model = model, forward_fn = forward_fn, verbose = 1)
  File "/ailab/public/pjlab-smarthealth03/leiwenhui/YushengTan/CLAM/extract_multi_features.py", line 38, in compute_w_loader
    for count, data in enumerate(tqdm(loader)):
  File "/ailab/user/leiwenhui/.conda/envs/clam/lib/python3.10/site-packages/tqdm/std.py", line 1181, in __iter__
    for obj in iterable:
  File "/ailab/user/leiwenhui/.conda/envs/clam/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 630, in __next__
    data = self._next_data()
  File "/ailab/user/leiwenhui/.conda/envs/clam/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 1324, in _next_data
    return self._process_data(data)
  File "/ailab/user/leiwenhui/.conda/envs/clam/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 1370, in _process_data
    data.reraise()
  File "/ailab/user/leiwenhui/.conda/envs/clam/lib/python3.10/site-packages/torch/_utils.py", line 706, in reraise
    raise exception
openslide.lowlevel.OpenSlideError: Caught OpenSlideError in DataLoader worker process 2.
Original Traceback (most recent call last):
  File "/ailab/user/leiwenhui/.conda/envs/clam/lib/python3.10/site-packages/torch/utils/data/_utils/worker.py", line 309, in _worker_loop
    data = fetcher.fetch(index)  # type: ignore[possibly-undefined]
  File "/ailab/user/leiwenhui/.conda/envs/clam/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py", line 52, in fetch
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/ailab/user/leiwenhui/.conda/envs/clam/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py", line 52, in <listcomp>
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/ailab/public/pjlab-smarthealth03/leiwenhui/YushengTan/CLAM/dataset_modules/dataset_h5.py", line 86, in __getitem__
    img = self.wsi.read_region(coord, self.patch_level, (self.patch_size, self.patch_size)).convert('RGB')
  File "/ailab/user/leiwenhui/.conda/envs/clam/lib/python3.10/site-packages/openslide/__init__.py", line 251, in read_region
    region = lowlevel.read_region(
  File "/ailab/user/leiwenhui/.conda/envs/clam/lib/python3.10/site-packages/openslide/lowlevel.py", line 335, in read_region
    _read_region(slide, buf, x, y, level, w, h)
  File "/ailab/user/leiwenhui/.conda/envs/clam/lib/python3.10/site-packages/openslide/lowlevel.py", line 233, in _check_error
    raise OpenSlideError(err)
openslide.lowlevel.OpenSlideError: OpenJPEG error: Expected a SOC marker

